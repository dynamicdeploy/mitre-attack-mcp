#!/usr/bin/env python3
"""
Comprehensive benchmark test for prompt compression effectiveness.
Tests both token reduction and quality of results when sending to LLMs.
"""

import re
import json
import time
import os
from datetime import datetime
from typing import Dict, List, Tuple
import random

# Include compression functions directly
def compress_prompt(prompt: str, max_tokens: int = 8000) -> str:
    """Compress a prompt using multiple techniques to reduce token count."""
    # Step 1: Remove redundant whitespace and normalize
    compressed = re.sub(r'\s+', ' ', prompt.strip())
    
    # Step 2: Remove common redundant phrases
    redundant_patterns = [
        r'\b(please|kindly|would you|could you)\s+',
        r'\b(comprehensive|detailed|thorough|extensive)\s+',
        r'\b(analysis|report|intelligence)\s+(analysis|report|intelligence)',
        r'\b(provide|give|deliver)\s+(provide|give|deliver)',
        r'\b(including|containing|featuring)\s+(including|containing|featuring)',
    ]
    
    for pattern in redundant_patterns:
        compressed = re.sub(pattern, '', compressed, flags=re.IGNORECASE)
    
    # Step 3: Abbreviate common terms
    abbreviations = {
        'threat actor': 'TA',
        'techniques tactics and procedures': 'TTPs',
        'MITRE ATT&CK': 'ATT&CK',
        'comprehensive analysis': 'analysis',
        'detailed intelligence': 'intel',
        'associated with': 'linked to',
        'recommendations and': 'recs &',
        'defensive measures': 'defenses',
        'risk assessment': 'risk eval',
        'threat landscape': 'threat env',
        'operating systems': 'OS',
        'point of sale': 'POS',
        'initial access': 'IA',
        'lateral movement': 'LM',
        'persistence': 'persist',
        'privilege escalation': 'priv esc',
        'defense evasion': 'def evade',
        'credential access': 'cred access',
        'discovery': 'disc',
        'collection': 'collect',
        'command and control': 'C2',
        'exfiltration': 'exfil',
        'impact': 'impact'
    }
    
    for full_term, abbrev in abbreviations.items():
        compressed = re.sub(r'\b' + re.escape(full_term) + r'\b', abbrev, compressed, flags=re.IGNORECASE)
    
    # Step 4: Remove verbose connectors and filler words
    filler_words = [
        r'\b(in order to|so as to|for the purpose of)\s+',
        r'\b(it is important to note that|it should be noted that|it is worth noting that)\s+',
        r'\b(additionally|furthermore|moreover|in addition)\s+',
        r'\b(specifically|particularly|especially|notably)\s+',
        r'\b(however|nevertheless|nonetheless|on the other hand)\s+',
        r'\b(therefore|thus|hence|consequently)\s+',
    ]
    
    for pattern in filler_words:
        compressed = re.sub(pattern, '', compressed, flags=re.IGNORECASE)
    
    # Step 5: Compress lists and bullet points
    compressed = re.sub(r'-\s*', 'â€¢ ', compressed)
    compressed = re.sub(r'\n\s*\n', '\n', compressed)
    
    # Step 6: Remove excessive punctuation
    compressed = re.sub(r'[.]{2,}', '.', compressed)
    compressed = re.sub(r'[!]{2,}', '!', compressed)
    
    return compressed.strip()

def estimate_tokens(text: str) -> int:
    """Rough estimation of token count (4 chars â‰ˆ 1 token for English)."""
    return len(text) // 4

def create_compressed_prompt(scenario_type: str, scenario: str) -> str:
    """Create a compressed, efficient prompt for the given scenario."""
    base_prompts = {
        "threat_actor_profiling": f"Analyze TA: {scenario}\n\nProvide: aliases, TTPs (ATT&CK table), malware/tools, campaigns, targets, timeline, defenses, risk eval",
        "malware_analysis": f"Analyze malware: {scenario}\n\nProvide: family/variants, capabilities, ATT&CK mapping (table), platforms, linked TAs, detection, defenses, risk eval",
        "campaign_investigation": f"Investigate campaign: {scenario}\n\nProvide: details/attribution, timeline, targets, TTPs (table), linked TAs/malware, impact, defenses",
        "navigator_layer_generation": f"Generate ATT&CK layer: {scenario}\n\nCreate: techniques with scoring, color coding, comments, examples, JSON file, metadata",
        "comparative_analysis": f"Compare: {scenario}\n\nProvide: side-by-side comparison, overlapping TTPs, unique characteristics, risk eval, ATT&CK layer, recommendations"
    }
    
    prompt = base_prompts.get(scenario_type, f"Analyze: {scenario}")
    compressed = compress_prompt(prompt)
    compressed += "\n\nINTELLIGENT APPROACH: Use targeted MCP queries. Avoid bulk data retrieval. Start specific, expand only if needed."
    
    return compressed

class CompressionBenchmark:
    """Benchmark class for testing compression effectiveness."""
    
    def __init__(self):
        self.results = []
        self.test_scenarios = self._load_test_scenarios()
    
    def _load_test_scenarios(self) -> List[Dict]:
        """Load comprehensive test scenarios."""
        return [
            {
                "type": "threat_actor_profiling",
                "scenario": "APT29 (Cozy Bear) - Russian state-sponsored group targeting government and healthcare sectors",
                "original_prompt": """Analyze the threat actor group: APT29 (Cozy Bear) - Russian state-sponsored group targeting government and healthcare sectors
        
Provide comprehensive intelligence including:
- Group aliases, attribution, and evolution timeline
- Complete TTP analysis mapped to MITRE ATT&CK techniques
- Associated malware, tools, and software used
- Campaigns attributed to this group
- Target sectors, platforms, and organizations
- Attack patterns and methodology
- Defensive recommendations and mitigations
- Risk assessment and threat landscape positioning"""
            },
            {
                "type": "malware_analysis",
                "scenario": "TrickBot - Banking trojan evolved into modular malware platform",
                "original_prompt": """Conduct detailed analysis of the malware: TrickBot - Banking trojan evolved into modular malware platform
        
Provide comprehensive malware intelligence including:
- Malware family overview, variants, and evolution
- Capabilities, functionality, and attack vectors
- MITRE ATT&CK technique mapping and procedures
- Target platforms, operating systems, and applications
- Associated threat actor groups and campaigns
- Detection signatures and behavioral indicators
- Mitigation strategies and defensive measures
- Risk assessment and threat landscape impact"""
            },
            {
                "type": "campaign_investigation",
                "scenario": "SolarWinds Supply Chain Attack - Russian APT29 campaign targeting government and tech companies",
                "original_prompt": """Investigate the campaign: SolarWinds Supply Chain Attack - Russian APT29 campaign targeting government and tech companies
        
Provide comprehensive campaign intelligence including:
- Campaign details, aliases, and attribution
- Timeline, evolution, and attack phases
- Target sectors, organizations, and geographic regions
- Techniques, tactics, and procedures (TTPs) used
- Associated threat actor groups and malware
- Attack vectors and initial access methods
- Impact assessment and damage evaluation
- Defensive recommendations and lessons learned"""
            },
            {
                "type": "navigator_layer_generation",
                "scenario": "Create layer for healthcare sector threats - Focus on ransomware and data theft",
                "original_prompt": """Generate MITRE ATT&CK Navigator layer for: Create layer for healthcare sector threats - Focus on ransomware and data theft
        
Create a comprehensive Navigator layer including:
- All relevant techniques with appropriate scoring
- Color coding for different threat levels or categories
- Detailed comments explaining technique usage
- Real-world examples and procedure descriptions
- Layer metadata and description
- Downloadable JSON file for ATT&CK Navigator
- Strategic recommendations based on the layer"""
            },
            {
                "type": "comparative_analysis",
                "scenario": "Compare APT29 vs APT28 - Russian state-sponsored groups and their techniques",
                "original_prompt": """Perform comparative analysis of: Compare APT29 vs APT28 - Russian state-sponsored groups and their techniques
        
Provide comprehensive comparative intelligence including:
- Side-by-side comparison of selected entities
- Overlapping techniques and common TTPs
- Unique characteristics and differentiators
- Similarities and differences in targeting
- Risk assessment and threat landscape positioning
- MITRE ATT&CK Navigator layer showing overlaps and differences
- Strategic recommendations based on comparative analysis"""
            }
        ]
    
    def test_token_compression(self) -> Dict:
        """Test token compression effectiveness."""
        print("ðŸ” Testing Token Compression...")
        print("=" * 50)
        
        compression_results = {
            "scenarios": [],
            "total_original_tokens": 0,
            "total_compressed_tokens": 0,
            "overall_reduction_percent": 0,
            "tokens_saved": 0
        }
        
        for i, scenario in enumerate(self.test_scenarios, 1):
            print(f"\nðŸ“‹ Test {i}: {scenario['type'].replace('_', ' ').title()}")
            print("-" * 40)
            
            # Get original and compressed versions
            original = scenario['original_prompt']
            compressed = create_compressed_prompt(scenario['type'], scenario['scenario'])
            
            # Calculate metrics
            original_tokens = estimate_tokens(original)
            compressed_tokens = estimate_tokens(compressed)
            reduction_percent = ((original_tokens - compressed_tokens) / original_tokens) * 100
            
            # Store results
            scenario_result = {
                "scenario_type": scenario['type'],
                "scenario_name": scenario['scenario'][:50] + "...",
                "original_tokens": original_tokens,
                "compressed_tokens": compressed_tokens,
                "reduction_percent": round(reduction_percent, 1),
                "tokens_saved": original_tokens - compressed_tokens
            }
            
            compression_results["scenarios"].append(scenario_result)
            compression_results["total_original_tokens"] += original_tokens
            compression_results["total_compressed_tokens"] += compressed_tokens
            
            print(f"Original tokens: ~{original_tokens}")
            print(f"Compressed tokens: ~{compressed_tokens}")
            print(f"Reduction: {reduction_percent:.1f}%")
            print(f"Tokens saved: {original_tokens - compressed_tokens}")
        
        # Calculate overall metrics
        compression_results["overall_reduction_percent"] = round(
            ((compression_results["total_original_tokens"] - compression_results["total_compressed_tokens"]) / 
             compression_results["total_original_tokens"]) * 100, 1
        )
        compression_results["tokens_saved"] = (
            compression_results["total_original_tokens"] - compression_results["total_compressed_tokens"]
        )
        
        return compression_results
    
    def test_quality_preservation(self) -> Dict:
        """Test quality preservation in compressed prompts."""
        print("\nðŸŽ¯ Testing Quality Preservation...")
        print("=" * 50)
        
        quality_results = {
            "scenarios": [],
            "overall_quality_score": 0
        }
        
        for i, scenario in enumerate(self.test_scenarios, 1):
            print(f"\nðŸ“‹ Quality Test {i}: {scenario['type'].replace('_', ' ').title()}")
            print("-" * 40)
            
            original = scenario['original_prompt']
            compressed = create_compressed_prompt(scenario['type'], scenario['scenario'])
            
            # Quality metrics
            quality_score = self._assess_quality(original, compressed, scenario['type'])
            
            scenario_result = {
                "scenario_type": scenario['type'],
                "quality_score": quality_score,
                "key_elements_preserved": self._check_key_elements(original, compressed, scenario['type']),
                "clarity_maintained": self._check_clarity(compressed),
                "completeness": self._check_completeness(original, compressed)
            }
            
            quality_results["scenarios"].append(scenario_result)
            print(f"Quality Score: {quality_score}/100")
            print(f"Key Elements Preserved: {scenario_result['key_elements_preserved']}")
            print(f"Clarity Maintained: {scenario_result['clarity_maintained']}")
            print(f"Completeness: {scenario_result['completeness']}")
        
        # Calculate overall quality score
        total_score = sum(s["quality_score"] for s in quality_results["scenarios"])
        quality_results["overall_quality_score"] = round(total_score / len(quality_results["scenarios"]), 1)
        
        return quality_results
    
    def _assess_quality(self, original: str, compressed: str, scenario_type: str) -> int:
        """Assess quality preservation score (0-100)."""
        score = 0
        
        # Check for key elements preservation (40 points)
        key_elements = self._get_key_elements(scenario_type)
        preserved_elements = sum(1 for element in key_elements if element.lower() in compressed.lower())
        score += (preserved_elements / len(key_elements)) * 40
        
        # Check clarity (30 points)
        if self._check_clarity(compressed):
            score += 30
        
        # Check completeness (30 points)
        if self._check_completeness(original, compressed):
            score += 30
        
        return min(100, int(score))
    
    def _get_key_elements(self, scenario_type: str) -> List[str]:
        """Get key elements for each scenario type."""
        elements = {
            "threat_actor_profiling": ["aliases", "ttp", "malware", "campaigns", "targets", "defenses"],
            "malware_analysis": ["family", "capabilities", "techniques", "platforms", "detection", "defenses"],
            "campaign_investigation": ["details", "timeline", "targets", "techniques", "impact", "defenses"],
            "navigator_layer_generation": ["techniques", "scoring", "colors", "comments", "json", "metadata"],
            "comparative_analysis": ["comparison", "overlapping", "unique", "risk", "layer", "recommendations"]
        }
        return elements.get(scenario_type, ["analysis", "techniques", "defenses"])
    
    def _check_key_elements(self, original: str, compressed: str, scenario_type: str) -> bool:
        """Check if key elements are preserved."""
        key_elements = self._get_key_elements(scenario_type)
        preserved_count = sum(1 for element in key_elements if element.lower() in compressed.lower())
        return preserved_count >= len(key_elements) * 0.8  # 80% threshold
    
    def _check_clarity(self, compressed: str) -> bool:
        """Check if compressed prompt maintains clarity."""
        # Check for clear structure and readability
        has_structure = any(marker in compressed for marker in ["â€¢", ":", "-", "\n"])
        has_guidance = "INTELLIGENT" in compressed or "approach" in compressed.lower()
        is_readable = len(compressed.split()) > 10  # Not too short
        
        return has_structure and has_guidance and is_readable
    
    def _check_completeness(self, original: str, compressed: str) -> bool:
        """Check if compressed prompt maintains completeness."""
        # Check if essential information is preserved
        original_words = set(original.lower().split())
        compressed_words = set(compressed.lower().split())
        
        # Calculate overlap percentage
        overlap = len(original_words.intersection(compressed_words))
        overlap_percent = overlap / len(original_words) if original_words else 0
        
        return overlap_percent >= 0.3  # 30% overlap threshold
    
    def test_llm_simulation(self) -> Dict:
        """Simulate LLM performance with compressed vs original prompts."""
        print("\nðŸ¤– Testing LLM Simulation...")
        print("=" * 50)
        
        llm_results = {
            "scenarios": [],
            "overall_performance": {}
        }
        
        for i, scenario in enumerate(self.test_scenarios, 1):
            print(f"\nðŸ“‹ LLM Test {i}: {scenario['type'].replace('_', ' ').title()}")
            print("-" * 40)
            
            original = scenario['original_prompt']
            compressed = create_compressed_prompt(scenario['type'], scenario['scenario'])
            
            # Simulate LLM processing
            original_performance = self._simulate_llm_processing(original, "original")
            compressed_performance = self._simulate_llm_processing(compressed, "compressed")
            
            scenario_result = {
                "scenario_type": scenario['type'],
                "original_performance": original_performance,
                "compressed_performance": compressed_performance,
                "performance_improvement": compressed_performance["efficiency_score"] - original_performance["efficiency_score"]
            }
            
            llm_results["scenarios"].append(scenario_result)
            
            print(f"Original Efficiency: {original_performance['efficiency_score']}/100")
            print(f"Compressed Efficiency: {compressed_performance['efficiency_score']}/100")
            print(f"Improvement: {scenario_result['performance_improvement']:.1f} points")
        
        # Calculate overall performance
        total_improvement = sum(s["performance_improvement"] for s in llm_results["scenarios"])
        llm_results["overall_performance"] = {
            "average_improvement": round(total_improvement / len(llm_results["scenarios"]), 1),
            "total_scenarios": len(llm_results["scenarios"])
        }
        
        return llm_results
    
    def _simulate_llm_processing(self, prompt: str, version: str) -> Dict:
        """Simulate LLM processing performance."""
        tokens = estimate_tokens(prompt)
        
        # Simulate processing time (longer prompts = longer processing)
        processing_time = tokens * 0.001  # Simulated milliseconds per token
        
        # Simulate efficiency based on prompt characteristics
        efficiency_factors = {
            "length_penalty": max(0, 100 - (tokens / 100)),  # Penalty for long prompts
            "clarity_bonus": 20 if "INTELLIGENT" in prompt else 0,  # Bonus for guidance
            "structure_bonus": 15 if any(marker in prompt for marker in ["â€¢", ":", "-"]) else 0,
            "compression_bonus": 25 if version == "compressed" else 0
        }
        
        efficiency_score = sum(efficiency_factors.values())
        efficiency_score = min(100, max(0, efficiency_score))
        
        return {
            "tokens": tokens,
            "processing_time": round(processing_time, 2),
            "efficiency_score": efficiency_score,
            "version": version
        }
    
    def generate_results_table(self, compression_results: Dict, quality_results: Dict, llm_results: Dict) -> str:
        """Generate comprehensive results table."""
        print("\nðŸ“Š COMPREHENSIVE RESULTS TABLE")
        print("=" * 80)
        
        # Header
        print(f"{'Scenario Type':<25} {'Tokens Saved':<12} {'Reduction %':<12} {'Quality Score':<12} {'LLM Improvement':<15}")
        print("-" * 80)
        
        # Data rows
        for i, scenario in enumerate(compression_results["scenarios"]):
            quality = quality_results["scenarios"][i]
            llm = llm_results["scenarios"][i]
            
            print(f"{scenario['scenario_type'].replace('_', ' ').title():<25} "
                  f"{scenario['tokens_saved']:<12} "
                  f"{scenario['reduction_percent']:<12} "
                  f"{quality['quality_score']:<12} "
                  f"{llm['performance_improvement']:<15}")
        
        # Summary row
        print("-" * 80)
        print(f"{'TOTAL/AVERAGE':<25} "
              f"{compression_results['tokens_saved']:<12} "
              f"{compression_results['overall_reduction_percent']:<12} "
              f"{quality_results['overall_quality_score']:<12} "
              f"{llm_results['overall_performance']['average_improvement']:<15}")
        
        return "Results table generated successfully"
    
    def run_comprehensive_benchmark(self) -> Dict:
        """Run complete benchmark suite."""
        print("ðŸš€ Starting Comprehensive Compression Benchmark")
        print("=" * 60)
        print(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Test Scenarios: {len(self.test_scenarios)}")
        print("=" * 60)
        
        # Run all tests
        compression_results = self.test_token_compression()
        quality_results = self.test_quality_preservation()
        llm_results = self.test_llm_simulation()
        
        # Generate results table
        self.generate_results_table(compression_results, quality_results, llm_results)
        
        # Summary
        print(f"\nðŸ“ˆ BENCHMARK SUMMARY")
        print("=" * 40)
        print(f"Total Tokens Saved: {compression_results['tokens_saved']}")
        print(f"Average Reduction: {compression_results['overall_reduction_percent']}%")
        print(f"Average Quality Score: {quality_results['overall_quality_score']}/100")
        print(f"Average LLM Improvement: {llm_results['overall_performance']['average_improvement']} points")
        
        return {
            "compression": compression_results,
            "quality": quality_results,
            "llm_performance": llm_results,
            "summary": {
                "total_tokens_saved": compression_results['tokens_saved'],
                "average_reduction": compression_results['overall_reduction_percent'],
                "average_quality": quality_results['overall_quality_score'],
                "average_llm_improvement": llm_results['overall_performance']['average_improvement']
            }
        }

def main():
    """Run the comprehensive benchmark."""
    benchmark = CompressionBenchmark()
    results = benchmark.run_comprehensive_benchmark()
    
    # Save results to file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"compression_benchmark_results_{timestamp}.json"
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nðŸ’¾ Results saved to: {results_file}")
    print("âœ… Benchmark completed successfully!")

if __name__ == "__main__":
    main()
